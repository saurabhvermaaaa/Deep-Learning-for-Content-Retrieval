\section{Solution Framework}
Fig.\ref{fig:Framework} shows a high level overview of the solution framework.
\begin{figure*}
\centering
\includegraphics[scale=0.45]{Framework}
\captionsetup{justification=centering}
\caption{Solution Framework}
\label{fig:Framework}
\end{figure*}

\subsection{Preprocessing}
We preprocess the data to make processing more meaningful \cite{weber2013finding}.
\begin{itemize}
\item\textbf{Filtering} Removal of markup, punctuation and special characters from sentences.
\item\textbf{Tokenization} Splitting of text into individual units.
\item\textbf{Stemming} Reduction of words to their base forms \cite{porter1980algorithm}.
\item\textbf{Stopwords removal} Deletion of words that do not convey any special meaning.
\item\textbf{Pruning} Removal of words that do appear with a low frequency throughout the text.
\end{itemize}
The result of these preprocessing steps is a set of feature words.

\subsection{Text Understanding}
Text understanding consists in reading texts formed in natural languages, determining the explicit or implicit meaning of each element such as words,phrases, sentences and paragraphs, and making inferences about the implicit or explicit properties of these texts\cite{linell2004written}. Text understanding can be handled by a deep learning system without artificially embedding knowledge about words, phrases, sentences or any other syntactic or semantic structures associated with a language.
ConvNets for text understanding are modular, where gradients are obtained by back-propagation to perform optimization.

\textbf{Key Modules:} It is a temporal convolutional module, which simply computes a 1-D convolution between input and output.

\textbf{Character Quantization:} Our model accepts a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding. Then, the sequence of characters is transformed into a sequence of such m sized vectors with fixed length l. Any character exceeding length l is ignored, and any characters that are not in the alphabet including blank characters is quantized as all-zero vectors.

\textbf{Model Design:} Models shall be tested with different number of hidden units and frame sizes to train model for optimal results. 
\begin{figure}
\centering
\includegraphics[scale=1]{DeepLearningModel}
\captionsetup{justification=centering}
\caption{Deep ConvNets model illustration for Feature extraction.}
\label{fig:DeepLearningModel}
\end{figure}

\cite{zhang2015character} \cite{zhang2015text} \cite{lecun1998gradient}.

\subsection{Clustering}
In recent years, internet has become a mainstream medium and offers opportunity for large-scale production and distribution. With more news than ever, it has become increasingly difficult to find relevant news. Regardless which approach is taken and which services are used, one may be confronted with multiple news about the same event within the field of interest. The importance of a news event creates the need for a regular detailed coverage and hence, duplicates and redundant pieces. During high-peak of interest to a particular topic, there is no imit to number of duplicates produced. We need to manually filter and review the relevant news pieces. Existing approaches like Weber et. al. \cite{weber2013finding} cluster news pieces based on similarity of textual content. We intend to use deep learning feature vectors for clustering news items into highly specific cluster from a particular news event. Clustering algorithm k-means\cite{telgarsky2010hartigan} does not work because it requires number of clusters beforehand. As the number of clusters will never be fixed, we use Average-link agglomerative clustering. We believe that the cluster should be densely connected to an event and thus, average-link distance.

\subsection{Popularity Prediction}
We are motivated to predict popularity of article beforehand only from content based features and store only a plausible set of articles from each cluster. Bandari et. al. \cite{bandari2012pulse} use a supervised classification of category of popularity based on number of tweets. We intend to generate a score for each of the articles unlike categorising them into particular classes.

\subsubsection{Features}
The choice of features is motivated by multiple qustions. Does the source agent reach many readers? Does the language connect with the reader? Has the article became outdated? Do we have some information in the news piece or not? Is the news worthy of a read? These questions helped us in designing following six features.

\begin{itemize}

\item\textbf{Age} The date of publication of news given by the dataset. We remove few records with missing dates.

\item\textbf{Text Quality} The ratio of size of document before and after preprocessing.

\item\textbf{Source Quality} The popularity of source of the content given by initial number of hits provided by the source. If missing, we use the popularity of news agent as a whole. This is log-normalized to account for high range of hits.

\item\textbf{Subjectivity} This examines whether an article is written in more emotional, touchy tone, where it connects with the reader. We make use of subjectivity classifier from Lingpipe, a natural language toolkit.

\item\textbf{Named Entities} We hypothesize that well-known named entities will cause a further spread of the article. For instance, articles on Narendra Modi are more likely to be popular among Indian Readers as compared to others. We make use of Stanford CoreNLP\cite{manning2014stanford} to process named entities. We rate entities based on their prominence or past popularity in the media.

\item\textbf{Factual Density} Previous works like Lex et. al. \cite{Lex:2012:MQW:2184305.2184308} suggest that density of factual content can be used to measure informativeness of text documents. Zhu et. al. \cite{zhu2014finding} try to eliminate redundancy in news pieces. We utilize the above works to rank pieces based on their information content.

\end{itemize}
